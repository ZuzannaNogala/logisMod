---
title: "A guide to projektROR"
output: rmarkdown::html_vignette
author: Hankus Oktawia, Nogala Zuzanna
vignette: >
  %\VignetteIndexEntry{A guide to projektROR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# About 

The projektROR is a R package with useful tools to analyze data using logistic model. 

Our package provides a method to build logistic regression based on user's data, which are `data.table` object. There are few methods to help user visualize data sets. The package cointent a boxplot method to show relationship between regresors and dependent variable. In addition user can see Correlation Heatmap. Fethermore user can compare models with implemented method of ROC curve with AUC or plot with fitted logit curve to data. Also package allows to compare regression with informative criterias like AIC, BIC and also by K-Fold Cross Validation. In addition, there is a method to computed all responese predictions for created model. 

In our package there are two datasets `citrus` and`creditData`, which are `data.table` objects. Based of them, we will show the funtions in projektROR package.

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r setup}
library(projektROR)
```

## 1. Visualize dataset

In this guide we will work with `citrus` dataset. This dataset contains 10000 observations. In variable `nameBin` there is a information if the fruit is an orange (0) or a grapefruit (1) based on features like diameter, weight and the RGB values of color. 

```{r, echo = TRUE}
str(citrus)
```

Firstly, let's create boxplots with using `visual_boxplot()` method. That's can help us to see relationship between features and `nameBin` variable, which we want to model.

```{r, echo = TRUE, fig.width=5, fig.height=4, fig.align='center'}
p1 <- visual_boxplot(data = citrus, str_name_Y = "nameBin", str_name_X = "diameter")
p1
```

If the user wants change the title, it is possibility to change it in an function's argument `plot_title`. 

```{r, echo=TRUE,  fig.width=5, fig.height=4}
p2 <- visual_boxplot(data = citrus, str_name_Y = "nameBin", 
                     str_name_X = "weight", plot_title = "weight vs nameBin")
```

Thanks to `patchwork` library, the plots can by display by using arithmetic operators like "+", "/". 

```{r, echo=TRUE,  fig.width=5, fig.height=4, fig.align='center'}
p2 <- visual_boxplot(data = citrus, str_name_Y = "nameBin", 
                     str_name_X = "weight", plot_title = "weight vs nameBin")
p3 <-visual_boxplot(data = citrus, str_name_Y = "nameBin", 
                    str_name_X = "red", plot_title = "red vs nameBin")
p4 <- visual_boxplot(data = citrus, str_name_Y = "nameBin", 
                     str_name_X = "green", plot_title= "green vs nameBin")
p5 <- visual_boxplot(data = citrus, str_name_Y = "nameBin", 
                     str_name_X = "blue", plot_title= "blue vs nameBin")

(p2 +p3) / (p4 + p5)
```

Second, user can take a look on correlation of all independent variables by displaying correlation Heatmap.

```{r, fig.width=5, fig.height=4,  fig.align='center'}
vars <- c("diameter", "weight", "red", "green", "blue", "nameBin")
corrHeatmap(citrus, vars)
```

## 2.Build the model

Firstly we want to build model with all of the variables. The user can build a model using `logisMod()` method. 

```{r}
allVarModel <- logisMod(formula = nameBin ~., data = citrus)
```

If we want to see the results we use base function `summary` or just print the model.

```{r}
summary(allVarModel)
```

It shows us that all of the variables are null, so we should consider different predictors. Lets see what we get for just "diameter" and "weight".

```{r, warning=FALSE}
dwModel <- logisMod(nameBin ~ diameter + weight, citrus)
summary(dwModel)
```

## 3. Choose the model

### 3.1 Comparing informative criterias 

One of the most important features of this package is possibility to compare all possible models for given vector of variables. It works with character vector of names of variables.

```{r}
Crit_dt <- countInfCrit(data = citrus, str = "nameBin", vars)
head(Crit_dt)
```

According to criterias, the best model is the one with the smallest value of AIC and BIC.

```{r, echo=FALSE}
Crit_dt[union(which.min(AIC), which.min(BIC)), ]
```

However, the criterias is good for small amount of predictors. For big set of regressors, the amount of models the method should check is growing and that's not optimal.

### 3.2 Deviance test

Another method to testing model is deviance Goodness-of-fit test named `deviance_test()`. Consider the testing problem:

$$H_0: \forall_{i \in I} \ \ \beta_i = 0 \ \ \ \textrm{against} \ \ \ H_1: \exists_{i \in I} \ \ \beta_i \neq 0,$$
where $I$ is a set of predictors' indexes. 

For example, test the significance of `red, green, blue` predictors. The model from alternative (`model_H1`) is "full model". Its regressors are the tested predictors and addition one (let's take model `nameBin ~  diameter + weight + red + green + blue`). On the other hand, the model from null hipothesis  (`model_H0`) is a "full model" without tested predictors (`nameBin ~  diameter + weight`).

The null hipothesis is recjeted when the value of `Deviance_statistic` is greater than `Critical_value`, which is a $1 - \alpha$ quantile from distribiution $\chi^2(|I|)$ or the `p_value` is smaller than $\alpha$.

```{r}
model_test <- logisMod(nameBin ~ diameter + weight, data = citrus)
model_full <- logisMod(nameBin ~ diameter + weight + red + green + blue, data = citrus)

deviance_test(model_H0 = model_test, model_H1 = model_full, alpha = 0.1)
```

### 3.3 ROC curve and AUC

```{r}
computeAUC(logisMod(nameBin ~ diameter + red + green + blue, data = citrus), "nameBin")
computeAUC(logisMod(nameBin ~ weight + red + green + blue, data = citrus), "nameBin")
computeAUC(logisMod(nameBin ~ diameter + weight + red + green + blue, data = citrus), "nameBin")
```

```{r, fig.height=4, fig.width=7}
thres_hold_seq <- seq(0,1, by = 0.05)

drawROC(thres_hold_seq, logisMod(nameBin ~ diameter + red + green + blue, data = citrus), "nameBin")
```

## 4. Plot response

Above examples show that this data set may not be perfect for logistic regression, but we may try to use another function. Lets plot predicted probabilities for one-variable model (we include intercept).

```{r, fig.height=4, fig.width=7}
oneVarModel <- logisMod(nameBin ~ red, citrus) 
resPlot(oneVarModel)
```

We don't have to create model to plot prediction. This function also works with data.table (or data.frame) and names of variables.

```{r, fig.height=4, fig.width=7}
resPlot(citrus, "nameBin", "diameter")
```

## 5. K-Fold Cross Validation

K-fold cross-validation technique is a method of resampling the data set in order to evaluate a machine learning model. In this technique, the parameter K refers to the number of different subsets that the given data set is to be split into. Further, K-1 subsets are used to train the model and the left out subsets are used as a validation set. For each subset we create model based on train data and count accuracy (ACC), then we get accuracy score by applying mean to all the accuracies received for all folds.

\[ACC =  \frac{\sum_{i=1}^n I(y_i - \hat{y}_i)}{n},\]

where $n$ is size of testing set. 

Below we use 4-Fold Cross Validation with threshold equal 0.5. 

```{r}
kFoldCV(citrus, 4, 0.5, "nameBin", c("red", "blue"))
```

We can also compare multiple thresholds at once, when we want to check which one is better. We pick threshold with bigger accuracy score.

```{r}
mulKFCV <- kFoldCV(citrus, 4, seq(0.1, 1, 0.1), "nameBin", c("red", "blue"))
mulKFCV
```

In this example we would choose:

```{r, echo = FALSE}
mulKFCV[accScore == max(accScore)]
```


