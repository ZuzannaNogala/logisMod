---
title: "A guide to projektROR"
output: rmarkdown::html_vignette
author: Hankus Oktawia, Nogala Zuzanna
vignette: >
  %\VignetteIndexEntry{A guide to projektROR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
# About 
  
The projektROR is a R package with useful tools to analyze data using logistic model. 

Our package provides a method to build logistic regression based on user's data, which are `data.table` object. There are few methods to help the user to visualize data sets. The package includes a boxplot method to show relationship between regressors and the dependent variable. In addition user can see Correlation Heatmap. Fethermore it is possible to compare models with implemented method of ROC curve with AUC or plot with fitted logit curve to data. Also package allows to compare regressions by informative criteria like AIC, BIC and also by K-Fold Cross Validation. In addition, there is a method to computed all responese predictions for created model's data or new one. 

In our package there are two datasets `citrus` and`creditData`, which are `data.table` objects. Based of them, we will show the funtions in projektROR package.

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r setup}
library(projektROR)
```

## 1. Visualize dataset

In this guide we will work with `citrus` dataset. This dataset contains 10000 observations. In variable `nameBin` there is a information if the fruit is an orange (0) or a grapefruit (1) based on features like diameter, weight and the RGB values of color. 

```{r, echo = TRUE}
str(citrus)
```

Firstly, let's create boxplots with using `visual_boxplot()` method. That's can help us to see relationship between features and `nameBin` variable, which we want to model.

```{r, echo = TRUE, fig.width=5, fig.height=4, fig.align='center'}
p1 <- visual_boxplot(data = citrus, strNameY = "nameBin", strNameX = "diameter")
p1
```

If the user wants change the title, it is possibility to change it in an function's argument `plot_title`. 

```{r, echo=TRUE,  fig.width=5, fig.height=4}
p2 <- visual_boxplot(data = citrus, strNameY = "nameBin", 
                     strNameX = "weight", plot_title = "weight vs nameBin")
```

Thanks to `patchwork` library, the plots can by display by using arithmetic operators like "+", "/". 

```{r, echo=TRUE,  fig.width=5, fig.height=4, fig.align='center'}
p2 <- visual_boxplot(data = citrus, strNameY = "nameBin", 
                     strNameX = "weight", plot_title = "weight vs nameBin")
p3 <-visual_boxplot(data = citrus, strNameY = "nameBin", 
                    strNameX = "red", plot_title = "red vs nameBin")
p4 <- visual_boxplot(data = citrus, strNameY = "nameBin", 
                     strNameX = "green", plot_title= "green vs nameBin")
p5 <- visual_boxplot(data = citrus, strNameY = "nameBin", 
                     strNameX = "blue", plot_title= "blue vs nameBin")

(p2 +p3) / (p4 + p5)
```

Second, user can take a look on correlation of all independent variables by displaying correlation Heatmap.

```{r, fig.width=5, fig.height=4,  fig.align='center'}
vars <- c("diameter", "weight", "red", "green", "blue", "nameBin")
corrHeatmap(citrus, vars)
```

## 2.Build the model

Firstly we want to build model with all of the variables. The user can build a model using `createModels()` method. 

```{r}
allVarModel <- createModels(citrus, nameBin ~.)
```

Thanks to it we can build multiple models at once, just by putting theirs formulas after commas. As a result, we always get a list of models (even if there is only one).

```{r}
fewModels <- createModels(citrus, nameBin ~ diameter, nameBin ~ weight, nameBin ~ red)
```

If we want to see the results we use base function `summary` or just print proper list element.

```{r}
summary(allVarModel[[1]])
```

It shows us that all of the variables are null, so we should consider different predictors. Lets see what we get for just "diameter" and "weight".

```{r, warning=FALSE}
dwModel <- createModels(citrus, nameBin ~ diameter + weight)
summary(dwModel[[1]])
```

## 3. Comparing informative criterias 

In our package there are few method to help the user choose a model. Firsty, method `countInfCrit()` computes values of criteria AIC and BIC for every possible set of predictors given by the user.

```{r}
Crit_dt <- countInfCrit(data = citrus, str = "nameBin", vars)
head(Crit_dt)
```

According to criteria, the best model is the one with the smallest value of AIC and BIC.

```{r, echo=TRUE}
Crit_dt[union(which.min(AIC), which.min(BIC)), ]
```

However, the criteria is good for small amount of predictors. For big set of regressors, the amount of models the method should check is growing and that's not optimal.

## 4. Deviance test

Another method to testing model is deviance goodness-of-fit test named `deviance_test()`. Consider the testing problem:
  
$$H_0: \forall_{i \in I} \ \ \beta_i = 0 \ \ \ \textrm{against} \ \ \ H_1: \exists_{i \in I} \ \ \beta_i \neq 0,$$
where $I$ is a set of predictors' indexes. 

For example, let's test the significance of `red, green, blue` predictors. The model from alternative is "full model". Its regressors are the tested predictors and addition one (let's take model `nameBin ~  diameter + weight + red + green + blue`). On the other hand, the model from null hypothesis is a "unfull model" without tested predictors (`nameBin ~  diameter + weight`).

The null hipothesis is recjeted when the value of `Deviance_statistic` is greater than `Critical_value`, which is a $1 - \alpha$ quantile from distribiution $\chi^2(|I|)$ or the `p_value` is smaller than $\alpha$.

```{r}
model_test <- createModels(data = citrus, nameBin ~ diameter + weight, 
                           nameBin ~ diameter + weight + red + green + blue)

deviance_test(model_test, alpha = 0.1)
```

`model_test` contains two models - one from null hypothesis and second from alternative. The order of adding models is not important.

### 5. ROC curve and AUC

A receiver operating characteristic curve (ROC curve) is a graphical plot which illustrates the performance of a binary classifier model for different choices of threshold of success' probability. Each point on curve represents False Positive Rate (FPR) against True Positive Rate (TPR) at each threshold setting. The AUC statistic (Area under curve) is used to compare ROC curves of different models. If AUC is closer to 1, the model performances better.
                                                                                                     In package those statistic are implemented. The user can display ROC curve plot for many logistic regressions using `drawROCsForEachModel()` method and compare AUC thanks to `computeAUC()` method. Second method is fixed form of `pROC::roc` function, which is usable for many logistic models at once. For more details or different version see [pROC::roc](https://www.rdocumentation.org/packages/pROC/versions/1.18.5/topics/roc).

```{r, fig.height=4, fig.width=7, echo=TRUE}
thres_hold_seq <- seq(0, 1, by = 0.05)
models_to_compare <- createModels(citrus, nameBin ~ diameter + red + green + blue,
nameBin ~ red + green + blue,
nameBin ~ diameter + weight + red + green + blue)
                                                                                                     
                                                                                                  computeAUC("nameBin", citrus, models_to_compare)
                                                                                                    drawROCsForEachModel(thres_hold_seq, "nameBin", models_to_compare)
```

## 6. Plot response

Above examples show that this data set may not be perfect for logistic regression, but we may try to use another function. Lets plot predicted probabilities for one-variable model (we include intercept).
 
```{r, echo=TRUE, fig.height=4, fig.width=7}
oneVarModel <- createModels(citrus, nameBin ~ red) 
resPlot(oneVarModel[[1]])
```

```{r, fig.height=4, fig.width=7}
resPlot(citrus, "nameBin", "diameter")
```

## 7. K-Fold Cross Validation

K-fold cross-validation technique is a method of resampling the data set in order to evaluate a machine learning model. In this technique, the parameter K refers to the number of different subsets that the given data set is to be split into. Further, K-1 subsets are used to train the model and the left out subsets are used as a validation set. For each subset we create model based on train data and count accuracy (ACC), then we get accuracy score by applying mean to all the accuracies received for all folds.

\[ACC =  \frac{\sum_{i=1}^n I(y_i - \hat{y}_i)}{n},\]

where $n$ is size of testing set. 

Below we use 4-Fold Cross Validation with threshold equal 0.5. 

```{r}
kFoldCV(citrus, 4, 0.5, "nameBin", c("red", "blue"))
```

We can also compare multiple thresholds at once, when we want to check which one is better. We pick threshold with bigger accuracy score.

```{r}
mulKFCV <- kFoldCV(citrus, 4, seq(0.1, 1, 0.1), "nameBin", c("red", "blue"))
mulKFCV
```

In this example we would choose:

```{r, echo = FALSE}
mulKFCV[.acc == max(.acc)]
```

## 8. Prediction of new values

We are creating models mostly beacuse we want to predict new values of dependent variable based on new observations. We provide function that can do this, it is fixed form of \code{stats::predict.glm}. For more details or different version see [stats::predict.glm()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/predict.glm).

```{r, eval=FALSE}
library(data.table)
model <- createModels(citrus, nameBin ~ red)
predData <- citrus[["red"]]
newData <- list()
newData[["red"]] <- seq(min(predData), max(predData), 0.1)
newData <- as.data.table(newData)

predictBasedData(model[[1]], newData)
```

## 9. Confusion / Error Matrix

Confusion / Error Matrix is table, which helps with visualization of model's performance. It contains values of True Positives, True Negative, False Positive and False Negatives. In package there is ` CreateErrorMatrixStats()` method, which returns the list with the Error Matrix, Accuracy, Specificity and Sensitivity of each model given by user in `models` argument. Beside this, in function there is `thres_prob` argument (threshold of success' probability) and `Y_vec` (values of dependent variable from model). They are useful to compute elements of Error matrix. 

```{r, echo=TRUE}
EMStats <- CreateErrorMatrixStats(thres_prob = 0.5, 
                                  Y_vec = citrus$nameBin, 
                                  models = models_to_compare)
 
EMStats$ErrorMatrix
EMStats$Specificity
```

Beside `data.frame` representation of Error Matrix, the user can display as an `ggplot2` object by using `visualErrorMatrix()` method. However, this function is usable for one model at once.

```{r, fig.height=4, fig.width=7}
visualErrorMatrix(thres_prob = 0.5, Y_vec = citrus$nameBin, model = models_to_compare[1])
visualErrorMatrix(thres_prob = 0.5, Y_vec = citrus$nameBin, model = models_to_compare[2])
```